{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project two ‚Äì character-level language modeling in PyTorch\n",
    "\n",
    "In the model that we will build now, the input is a text document, and our goal is to develop a model that can generate new text that is similar in style to the input document. Examples of such input are a book or a computer program in a specific programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 1112350\n",
      "Unique characters: 80\n"
     ]
    }
   ],
   "source": [
    "# Reading and processing text\n",
    "with open('book.txt', 'r', encoding='utf8') as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Total length:', len(text))\n",
    "print('Unique characters:', len(char_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text encoded shape: (1112350,)\n",
      "THE MYSTERIOUS  == Encoding ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28] == Reverse ===> ISLAND\n"
     ]
    }
   ],
   "source": [
    "# Building the dictionary to map characters to integers, and reverse mapping via indexing a NumPy array\n",
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch: i for i, ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32\n",
    ")\n",
    "print('text encoded shape:', text_encoded.shape)\n",
    "print(text[:15], '== Encoding ==>', text_encoded[:15])\n",
    "print(text_encoded[15: 21], '== Reverse ===>',\n",
    "      ''.join(char_array[text_encoded[15:21]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 -> T\n",
      "32 -> H\n",
      "29 -> E\n",
      "1 ->  \n",
      "37 -> M\n"
     ]
    }
   ],
   "source": [
    "# print out the mappings of the first five characters from this array\n",
    "for c in text_encoded[:5]:\n",
    "    print('{} -> {}'.format(c, char_array[c]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "text_chunks = [text_encoded[i: i+chunk_size]\n",
    "               for i in range(len(text_encoded) - chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
      "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51 74] ---> ['T' 'H' 'E' ' ' 'M' 'Y' 'S' 'T' 'E' 'R' 'I' 'O' 'U' 'S' ' ' 'I' 'S' 'L'\n",
      " 'A' 'N' 'D' ' ' '*' '*' '*' '\\n' '\\n' '\\n' '\\n' '\\n' 'P' 'r' 'o' 'd' 'u'\n",
      " 'c' 'e' 'd' ' ' 'b' 'y']\n",
      "\n",
      "[32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6  6\n",
      "  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51 74  1] ---> ['H' 'E' ' ' 'M' 'Y' 'S' 'T' 'E' 'R' 'I' 'O' 'U' 'S' ' ' 'I' 'S' 'L' 'A'\n",
      " 'N' 'D' ' ' '*' '*' '*' '\\n' '\\n' '\\n' '\\n' '\\n' 'P' 'r' 'o' 'd' 'u' 'c'\n",
      " 'e' 'd' ' ' 'b' 'y' ' ']\n",
      "\n",
      "[29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6  6  0\n",
      "  0  0  0  0 40 67 64 53 70 52 54 53  1 51 74  1 25] ---> ['E' ' ' 'M' 'Y' 'S' 'T' 'E' 'R' 'I' 'O' 'U' 'S' ' ' 'I' 'S' 'L' 'A' 'N'\n",
      " 'D' ' ' '*' '*' '*' '\\n' '\\n' '\\n' '\\n' '\\n' 'P' 'r' 'o' 'd' 'u' 'c' 'e'\n",
      " 'd' ' ' 'b' 'y' ' ' 'A']\n",
      "\n",
      "[ 1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6  6  0  0\n",
      "  0  0  0 40 67 64 53 70 52 54 53  1 51 74  1 25 63] ---> [' ' 'M' 'Y' 'S' 'T' 'E' 'R' 'I' 'O' 'U' 'S' ' ' 'I' 'S' 'L' 'A' 'N' 'D'\n",
      " ' ' '*' '*' '*' '\\n' '\\n' '\\n' '\\n' '\\n' 'P' 'r' 'o' 'd' 'u' 'c' 'e' 'd'\n",
      " ' ' 'b' 'y' ' ' 'A' 'n']\n",
      "\n",
      "[37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6  6  0  0  0\n",
      "  0  0 40 67 64 53 70 52 54 53  1 51 74  1 25 63 69] ---> ['M' 'Y' 'S' 'T' 'E' 'R' 'I' 'O' 'U' 'S' ' ' 'I' 'S' 'L' 'A' 'N' 'D' ' '\n",
      " '*' '*' '*' '\\n' '\\n' '\\n' '\\n' '\\n' 'P' 'r' 'o' 'd' 'u' 'c' 'e' 'd' ' '\n",
      " 'b' 'y' ' ' 'A' 'n' 't']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(text_chunks[i], '--->', char_array[text_chunks[i]])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\promar\\AppData\\Local\\Temp\\ipykernel_4684\\3657071734.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_chunk = self.text_chunks[index]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "\n",
    "\n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x):  'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "Target (y):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      " Input (x):  'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "Target (y):  'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let‚Äôs take a look at some example sequences from this transformed dataset:\n",
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input (x): ', repr(''.join(char_array[seq])))\n",
    "    print('Target (y): ', repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform this dataset into mini-batches\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size,\n",
    "                    shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a character-level RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(embed_dim, self.rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model parameters and create an RNN model\n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # Multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3719\n",
      "Epoch 500 loss: 1.5092\n",
      "Epoch 1000 loss: 1.3639\n",
      "Epoch 1500 loss: 1.3214\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / seq_length\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation phase ‚Äì generating new text passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
    "    encoded_input = torch.tensor(\n",
    "        [char2int[s] for s in starting_str]\n",
    "    ).reshape((1, -1))\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
    "\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island is necessary fire, wrutilay?‚Äù\n",
      "\n",
      "‚ÄúYough!‚Äù cried Herbert.\n",
      "\n",
      "‚ÄúYes; and it had happen out sown yourselves?‚Äù\n",
      "\n",
      "‚ÄúThey would then twelve great several some day,‚Äù replied Herbert.\n",
      "The engineer calculated by their trees. There could\n",
      "exactle tree would let his eyes had to see the fiber but of course superous\n",
      "two sincure, when better that coal, and which he touched for surveyed accose the\n",
      "facatives, and in the matters, by a hear the cavern, and the sailor lad?‚Äù\n",
      "\n",
      "‚ÄúWere shoulded, any signonists!‚Äù\n",
      "\n",
      "No, for the \n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling factor, ùõº, can be interpreted as an analog to the temperature in physics. Higher temperatures result in more\n",
    "entropy or randomness versus more predictable behavior at lower temperatures. By scaling the logits\n",
    "with ùõº<1, the probabilities computed by the softmax function become more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities before scaling:         [0.10650698 0.10650698 0.78698605]\n",
      "Probabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611686]\n",
      "Probabilities after scaling with 0.1: [0.3104238  0.3104238  0.37915248]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "print('Probabilities before scaling:        ',\n",
    "      nn.functional.softmax(logits, dim=1).numpy()[0])\n",
    "print('Probabilities after scaling with 0.5:',\n",
    "      nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
    "print('Probabilities after scaling with 0.1:',\n",
    "      nn.functional.softmax(0.1*logits, dim=1).numpy()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large ùõº --> more predictabel\n",
    "\n",
    "small ùõº --> more randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island of the plateau of this season would be a leader of the southeast, and the sailor was not yet the surface of the balloon, which resture the depth of the forest, and became merculation of the beach, and it was not to be no doubted to devote them the forest, and the settlers had been brought the summit of the mountain was not time his master of the single of the balloon, captain,\n",
      "and the pastaways, which would have been stones, he will not be a part of the colonists, and the sailor was only the po\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=2.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island\n",
      "call is, helf and my‚Äùlnea-‚Äùime.5‚Äô‚Äù\n",
      "thed Skiletcain, to,--w rub.\n",
      "He, Separent ov at they! ‚Äùe loodryswelling,‚Äù woudJ?‚Äù\n",
      "\n",
      "This enclosmoy, Nock Nemied Thip‚Äôs recook! Wid fat\n",
      "will heetpienc? Con\n",
      "Palk, hy my\n",
      "Ovide?‚Äù osand.\n",
      "2xouait fur dutic wlickingly, it was in\n",
      "arraughirn. He\n",
      "regars nearctscriets\n",
      "ton‚Äô&lupus, Propnivic.\n",
      "\n",
      "Iveryath quap, roes, xitch. Withouth wash lrove.., Enewable\n",
      "objac. Horure my, Tippstahs, huspeye clear rippects. No statts sfacbed!\n",
      "\n",
      "Dubchify. The ify was,‚Äù saies off?, Ebbuired, perp\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "print(sample(model, starting_str='The island', scale_factor=0.5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4ffc9c9a031070cdf645d18822cd1abb711111454341ec96112cbb04136171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
