{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project one â€“ predicting the sentiment of IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDB(split='train')\n",
    "test_dataset = IMDB(split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    list(train_dataset), [20000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos',\n",
       " 'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: find unique tokens (words)\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab-size: 69023\n"
     ]
    }
   ],
   "source": [
    "tokens_count = Counter()\n",
    "for label, line in train_dataset:\n",
    "    tokens = tokenizer(line)\n",
    "    tokens_count.update(tokens)\n",
    "\n",
    "print('vocab-size:', len(tokens_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'was', 'talked', 'about', 'to']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: encoding each unique token into integers\n",
    "from torchtext.vocab import vocab\n",
    "sorted_by_freq_tuples = sorted(tokens_count.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(ordered_dict)\n",
    "vocab.insert_token('<pad>', 0)\n",
    "vocab.insert_token('<unk>', 1)\n",
    "vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 7, 35, 457]\n"
     ]
    }
   ],
   "source": [
    "# convert an example input text into a list of integer values\n",
    "print([vocab[token] for token in ['this', 'is', 'an', 'example']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-A: define the functions for transformation\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x=='pos' else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3-B: wrap the encode and transformation function\n",
    "\n",
    "def collate_batch(batch):\n",
    "    lbl_lst, txt_lst, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        lbl_lst.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        txt_lst.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    \n",
    "    lbl_lst = torch.tensor(lbl_lst)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    padded_text_list = nn.utils.rnn.pad_sequence(txt_lst, batch_first=True)\n",
    "    return padded_text_list, lbl_lst, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a small batch\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide all three datasets into data loaders with a batch size of 32\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2902,  0.2566, -0.2339],\n",
      "         [ 0.1644,  0.9498, -0.0640],\n",
      "         [ 1.1612,  0.8091, -0.2493],\n",
      "         [ 1.9732, -1.6155,  0.7387]],\n",
      "\n",
      "        [[ 1.1612,  0.8091, -0.2493],\n",
      "         [ 0.1517,  0.0105, -0.3540],\n",
      "         [ 0.1644,  0.9498, -0.0640],\n",
      "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=3, padding_idx=0)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "txt_encoded_input = torch.LongTensor([\n",
    "    [1, 2, 4, 5],\n",
    "    [4, 3, 2, 0]\n",
    "])\n",
    "print(embedding(txt_encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an RNN model\n",
    "\n",
    "we will create an RNN model with two recurrent layers of type RNN. Finally, we will add a non-recurrent fully connected layer as the output layer, which will return a single output value as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, hidden = self.rnn(x)\n",
    "        # we use the final hidden state from the last hidden layer as the input to the fully connected layer\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN(64, 32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2987],\n",
       "        [-0.1265],\n",
       "        [-0.0386],\n",
       "        [ 0.3542],\n",
       "        [-0.1470]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(5, 3, 64)) #batch size = 5, sequence length = 3, each sequence has 64 input features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an RNN model for the sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=rnn_hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "        out, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(69025, 20, padding_idx=0)\n",
       "  (rnn): LSTM(20, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 64\n",
    "fc_hidden_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for txt_batch, lbl_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(txt_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, lbl_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == lbl_batch).float().sum().item()\n",
    "        total_loss += loss.item() * lbl_batch.size(0)\n",
    "    \n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for txt_batch, lbl_batch, lengths in dataloader:\n",
    "            pred = model(txt_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, lbl_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == lbl_batch).float().sum().item()\n",
    "            total_loss += loss.item() * lbl_batch.size(0)\n",
    "    \n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy: 0.7609 val_accuracy: 0.7542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\courses trainig folders\\Python Machine Learning\\Python-Machine-Learning-Notebooks\\CH 15 - Modeling Sequential Data Using Recurrent Neural Networks\\Project 1.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     acc_train, loss_train \u001b[39m=\u001b[39m train(train_dl)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     acc_valid, loss_valid \u001b[39m=\u001b[39m evaluate(valid_dl)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc_train\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m val_accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc_valid\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mf:\\courses trainig folders\\Python Machine Learning\\Python-Machine-Learning-Notebooks\\CH 15 - Modeling Sequential Data Using Recurrent Neural Networks\\Project 1.ipynb Cell 27\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m pred \u001b[39m=\u001b[39m model(txt_batch, lengths)[:, \u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, lbl_batch)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/courses%20trainig%20folders/Python%20Machine%20Learning/Python-Machine-Learning-Notebooks/CH%2015%20-%20Modeling%20Sequential%20Data%20Using%20Recurrent%20Neural%20Networks/Project%201.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m total_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m ((pred \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mfloat() \u001b[39m==\u001b[39m lbl_batch)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\promar\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\promar\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "num_epochs = 3\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f}' f' val_accuracy: {acc_valid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test, loss_test = evaluate(test_dl)\n",
    "print(f'accuracy: {acc_test:.4f}' f' loss: {loss_test:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM passes through the input sequences from start to end and in reverse direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embed_dim=embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
    "        self.relu1 = nn.Relu()\n",
    "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, text, lengths):\n",
    "        out = self.embedding(text)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(\n",
    "            out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n",
    "        )\n",
    "\n",
    "        _, (hidden, cell) = self.rnn(out)\n",
    "        out = torch.cat(\n",
    "            (hidden[-2, :, :], hidden[-1, :, :]),\n",
    "            dim=1\n",
    "        )\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "torch.manual_seed(1)\n",
    "bi_model = Bi_RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
    "bi_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_train(dataloader):\n",
    "    bi_model.train()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    for txt_batch, lbl_batch, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = bi_model(txt_batch, lengths)[:, 0]\n",
    "        loss = loss_fn(pred, lbl_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += ((pred >= 0.5).float() == lbl_batch).float().sum().item()\n",
    "        total_loss += loss.item() * lbl_batch.size(0)\n",
    "    \n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    bi_model.eval()\n",
    "    total_acc, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for txt_batch, lbl_batch, lengths in dataloader:\n",
    "            pred = bi_model(txt_batch, lengths)[:, 0]\n",
    "            loss = loss_fn(pred, lbl_batch)\n",
    "            total_acc += ((pred >= 0.5).float() == lbl_batch).float().sum().item()\n",
    "            total_loss += loss.item() * lbl_batch.size(0)\n",
    "    \n",
    "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch} accuracy: {acc_train:.4f}' f' val_accuracy: {acc_valid:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4ffc9c9a031070cdf645d18822cd1abb711111454341ec96112cbb04136171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
