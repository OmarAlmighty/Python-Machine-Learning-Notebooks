{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the PyTorch Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "\n",
    "        # new PL attributes:\n",
    "        self.train_acc = Accuracy()\n",
    "        self.valid_acc = Accuracy()\n",
    "        self.test_acc = Accuracy()\n",
    "\n",
    "        # Model similar to previous section:\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        all_layers = [nn.Flatten()]\n",
    "        for unit in hidden_units:\n",
    "            layer = nn.Linear(input_size, unit)\n",
    "            all_layers.append(layer)\n",
    "            all_layers.append(nn.ReLU())\n",
    "            input_size = unit\n",
    "\n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        self.log('Train_acc', self.train_acc.compute())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y)\n",
    "        self.log('Valid_loss', loss, prog_bar=True)\n",
    "        self.log('Valid_acc', self.valid_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log('Test_loss', loss, prog_bar=True)\n",
    "        self.log('Test_acc', self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data loaders for Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='./'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path, download=False)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        # here note relevant\n",
    "        mnist_all = MNIST(\n",
    "            root=self.data_path,\n",
    "            train=True,\n",
    "            transform=self.transform,\n",
    "            download=False\n",
    "        )\n",
    "        self.train, self.val = random_split(\n",
    "            mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(1))\n",
    "        self.test = MNIST(\n",
    "            root=self.data_path,\n",
    "            train=False,\n",
    "            transform=self.transform,\n",
    "            download=False\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "mnist_dm = MnistDataModule()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model using the PyTorch Lightning Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: f:\\courses trainig folders\\Python Machine Learning\\Python-Machine-Learning-Notebooks\\The Mechanics of PyTorch\\lightning_logs\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | train_acc | Accuracy   | 0     \n",
      "1 | valid_acc | Accuracy   | 0     \n",
      "2 | test_acc  | Accuracy   | 0     \n",
      "3 | model     | Sequential | 25.8 K\n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 939/939 [00:31<00:00, 29.97it/s, loss=0.1, v_num=0, train_loss=0.260, Valid_loss=0.166, Valid_acc=0.936]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 939/939 [00:31<00:00, 29.96it/s, loss=0.1, v_num=0, train_loss=0.260, Valid_loss=0.166, Valid_acc=0.936]\n"
     ]
    }
   ],
   "source": [
    "mnist_clf = MultiLayerPerceptron()\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model=mnist_clf, datamodule=mnist_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7164), started 3:27:51 ago. (Use '!kill 7164' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d5183965268e7e41\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d5183965268e7e41\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard - -logdir lightning_logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\promar\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\checkpoint_connector.py:52: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\promar\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:731: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v2.0. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n",
      "  ckpt_path = ckpt_path or self.resume_from_checkpoint\n",
      "Restoring states from the checkpoint path at ./lightning_logs/version_0/checkpoints/epoch=9-step=8600.ckpt\n",
      "c:\\Users\\promar\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:345: UserWarning: The dirpath has changed from 'f:\\\\courses trainig folders\\\\Python Machine Learning\\\\Python-Machine-Learning-Notebooks\\\\The Mechanics of PyTorch\\\\lightning_logs\\\\version_0\\\\checkpoints' to 'f:\\\\courses trainig folders\\\\Python Machine Learning\\\\Python-Machine-Learning-Notebooks\\\\The Mechanics of PyTorch\\\\lightning_logs\\\\version_1\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | train_acc | Accuracy   | 0     \n",
      "1 | valid_acc | Accuracy   | 0     \n",
      "2 | test_acc  | Accuracy   | 0     \n",
      "3 | model     | Sequential | 25.8 K\n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at ./lightning_logs/version_0/checkpoints/epoch=9-step=8600.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 939/939 [00:31<00:00, 29.42it/s, loss=0.0762, v_num=1, train_loss=0.158, Valid_loss=0.168, Valid_acc=0.941]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 939/939 [00:31<00:00, 29.40it/s, loss=0.0762, v_num=1, train_loss=0.158, Valid_loss=0.168, Valid_acc=0.941]\n"
     ]
    }
   ],
   "source": [
    "# Lightning allows us to load a trained model and train it for additional epochs\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15, resume_from_checkpoint='./lightning_logs/version_0/checkpoints/epoch=9-step=8600.ckpt')\n",
    "trainer.fit(model=mnist_clf, datamodule=mnist_dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:02<00:00, 65.56it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        Test_acc            0.9517738223075867\n",
      "        Test_loss           0.14718946814537048\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test_loss': 0.14718946814537048, 'Test_acc': 0.9517738223075867}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "trainer.test(model=mnist_clf, datamodule=mnist_dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning also saves the model automatically\n",
    "# model = MultiLayerPerceptron.load_from_checkpoint(\"./lightning_logs/version_0/checkpoints/epoch=9-step=8600.ckpt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b4ffc9c9a031070cdf645d18822cd1abb711111454341ec96112cbb04136171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
